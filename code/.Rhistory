import numpy as np
import pandas as pd
import tweepy
list(1,2,3)
list([1,2,3])
x = list([1,2,3])
y = list([2,3,4])
import matplotlib
import matplotlib.pyplot as plt
b
import matplotlib
import matplotlib.pyplot as plt
t = np.arange(0.0, 2.0, 0.01)
s = 1 + np.sin(2 * np.pi * t)
fig, ax = plt.subplots()
ax.plot(t, s)
ax.set(xlabel='time (s)', ylabel='voltage (mV)',
ax.set(xlabel='time (s)', ylabel='voltage (mV)', title='About as simple as it gets, folks')
ax.grid()
ax.set(xlabel='time (s)', ylabel='voltage (mV)', title='About as simple as it gets, folks')
ax.set(xlabel='time (s)', ylabel='voltage (mV)', title='About as simple as it gets, folks')
ax.grid()
fig.savefig("test.png")
plt.show()
library("guanteda")
library("quanteda")
library("quanteda")
reticulate::repl_python()
reticulate::repl_python()
import tweepy
reticulate::repl_python()
import tweepy
import tweepy
reticulate::install_miniconda()
reticulate::repl_python()
conda_install("openpyxl")
library(reticulate)
conda_install("openpyxl")
reticulate::repl_python()
virtualenv_install("openpyxl")
require(shiny)
shiny::runGitHub(“potential_outcomes_test”, “gperrett”,  launch.browser = TRUE)
shiny::runGitHub("potential_outcomes_test", "gperrett", launch.brower = True)
shiny::runGitHub("potential_outcomes_test", "gperrett", launch.browser = True)
shiny::runGitHub("potential_outcomes_test", "gperrett", launch.browser = True)
install.packages("shinyjs")
shiny::runGitHub("potential_outcomes_test", "gperrett", launch.browser = True)
r.version()
R.version()
version
library(shiny)
shiny::runGitHub("potential_outcomes_test", "gperrett",  launch.browser = TRUE)
shiny::runGitHub("potential_outcomes_test", "gperrett",  launch.browser = TRUE)
devtools::install_github('joemarlo/plotBart')
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
install.packages('readstata13')
devtools::install_github('joemarlo/plotBart')
devtools::install_github('joemarlo/plotBart')
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
install.packages('shinyWidgets')
install.packages("shinyWidgets")
install.packages("shinyWidgets")
install.packages("shinyWidgets")
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
install.packages('bslib')
devtools::install_github('joemarlo/plotBart')
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
devtools::install_github('joemarlo/plotBart', force = TRUE)
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
install.packages('bslib', force = TRUE)
update.packages("fastmap")
install.packages('bslib', force = TRUE)
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
library('shinyWidgets')
shiny::runGitHub("thinkCausal_dev", "gperrett", subdir = 'Development', launch.browser = TRUE)
install.packages('bslib')
install.packages("sass", type="source")
packages <- c('shiny', 'foreign', 'readstata13', 'openxlsx', 'shinyjs', 'shinyWidgets', 'DT', 'kableExtra', 'sortable', 'tidyverse', 'patchwork', 'viridis', 'bartCause')
install.packages(packages)
install.packages("devtools")
install.packages("rpart.plot")
install.packages("gtools")
install.packages(packages)
install.packages("devtools")
install.packages("devtools")
#Clearing the environment
rm(list = ls())
# Setting the current working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#import libraries
library(dplyr)
library(quanteda)
library(quanteda.textstats)
library(quanteda.corpora)
library(quanteda.textmodels)
library(ggplot2)
library(topicmodels)
library(stm)
library(ggpubr)
# read in file
ukraine <- read.csv("../data/ukraine_tweets.csv") %>% mutate(id_str = as.character(id_str))
ukraine$text <- gsub(pattern = "'", "", ukraine$text)  # replace apostrophes
#format as date, remove Independents
ukraine <- ukraine %>% mutate(created_at = as.Date(created_at, format = "%Y-%m-%d")) %>%
filter(grepl('D|R', Party))
#form df to plot tweets over time
to_plot <- ukraine %>% group_by(Party, created_at) %>%
filter(created_at >= '2022-01-06 ') %>% count()
#plot tweets over time
ggplot(to_plot, aes(created_at, n)) +
geom_line() +
facet_grid(Party ~ .) +
ylab('Number of Tweets') +
ggtitle("Tweets per day by Party") +
theme_bw()
#clean up text
ukraine$text <- gsub(" https.*","",ukraine$text)
ukraine$text <- gsub("https.*","",ukraine$text)
# Remove non ASCII characters
ukraine$text <- stringi::stri_trans_general(ukraine$text, "latin-ascii")
# Removes solitary letters
ukraine$text <- gsub(" [A-z] ", " ", ukraine$text)
#create binary
ukraine <- ukraine %>% mutate(Party_binary = case_when(Party == 'D' ~ 1, Party == 'R' ~ 0))
#textProcessor to prepare for stm
temp <- textProcessor(documents=ukraine$text, metadata = ukraine)
meta<-temp$meta
vocab<-temp$vocab
docs<-temp$documents
#prepDocuments to prepare for stm
prepped <- prepDocuments(documents = docs, meta = meta, vocab = vocab, lower.thresh = 30)
docs <- prepped$documents
vocab <- prepped$vocab
meta <- prepped$meta
stm_40 <- stm(docs, vocab, K = 40, prevalence = ~meta$Party_binary + s(meta$created_at), content = meta$Party_binary)
#build 40 topic stm model for use in regression
#commented out because it's already built and saved
#can just load from below
#stm_40 <- stm(docs, vocab, K = 40, prevalence = ~meta$Party_binary + s(meta$created_at), content = meta$Party_binary)
saveRDS(stm_40, file = paste0(getwd(), '/virality_stm.rds'))
#build 40 topic stm model for use in regression
#commented out because it's already built and saved
#can just load from below
#stm_40 <- stm(docs, vocab, K = 40, prevalence = ~meta$Party_binary + s(meta$created_at), content = meta$Party_binary)
#saveRDS(stm_40, file = paste0(getwd(), '/virality_stm.rds'))
stm_viral <- readRDS(paste0(getwd(), '/virality_stm.rds'))
stm_viral
stm_viral$mu
stm_viral$sigma
stm_viral$beta
findThoughts(stm_viral)
findThoughts(stm_viral, texts = meta$text)
findThoughts(stm_viral, texts = meta$text, n = 2)
findThoughts(stm_viral, texts = meta$text, n = 1)
stm_viral$theta
apply(stm_viral$theta, MARGIN=1, FUN=which.max)
topic_doc = apply(stm_viral$theta, MARGIN=1, FUN=which.max)
meta$top_topic = topic_doc
View(meta)
meta %>% select(viral, top_topic)
for_model <- meta %>% select(viral, top_topic)
for_model %>% mutate(value = 1)  %>% spread(top_topic, value,  fill = 0 )
for_model %>% mutate(value = 1)  %>% tidyr::spread(top_topic, value,  fill = 0 )
for_model <- meta %>% select(viral, top_topic) %>% mutate(value = 1)  %>% tidyr::spread(top_topic, value,  fill = 0 )
for_model %>% mutate(value = 1)  %>% tidyr::spread(top_topic, value,  fill = 0 )
library(caret)
for_model$top_topic <- as.factor(for_model$top_topic)
one_hot(as.data.table(data))
library(mltools)
install.packages('mltools')
library(mltools)
one_hot(as.data.table(data))
one_hot(as.data.table(for_model))
one_hot(model)
one_hot(for_model)
library(caret)
dummy <- dummyVars(" ~ .", data=for_model)
data.frame(predict(dummy, newdata = data))
dummy <- dummyVars(" ~ .", data=for_model)
data.frame(predict(dummy, newdata = data))
data.frame(predict(dummy, newdata = for_model))
x <- data.frame(predict(dummy, newdata = for_model))
View(x)
for_model <- data.frame(predict(dummy, newdata = for_model))
View(for_model)
lm(viral~.,for_model)
model <- lm(viral~.,for_model)
model$coefficients
summ(model)
library(jtools)
install.packages('jtools')
sum(model)
install.packages("jtools")
summary(model)
#linear model, does
model <- glm(viral~.,for_model)
#linear model, does
model <- glm(viral~.,for_model, family = 'binomial')
summary(model)
#linear model, does topic pretty virality
model <- lm(viral~.,for_model)
summary(model)
View(meta)
#select only these two cols
for_model <- meta %>% select(viral, top_topic, Party)
#one hot encodes the topic assignment
for_model$top_topic <- as.factor(for_model$top_topic)
dummy <- dummyVars(" ~ .", data=for_model)
for_model <- data.frame(predict(dummy, newdata = for_model))
View(model)
for_model <- data.frame(predict(dummy, newdata = for_model))  %>%
select(Party.I == 0)
#select only these two cols
for_model <- meta %>% select(viral, top_topic, Party)
#one hot encodes the topic assignment
for_model$top_topic <- as.factor(for_model$top_topic)
dummy <- dummyVars(" ~ .", data=for_model)
for_model <- data.frame(predict(dummy, newdata = for_model))  %>%
select(Party.I == 0)
for_model <- data.frame(predict(dummy, newdata = for_model))
for_model %>%select('Party.I' == 0)
for_model %>%filter('Party.I' == 0)
for_model <- data.frame(predict(dummy, newdata = for_model)) %>%
filter('Party.I' == 0) %>%
select(-Party.R)
e two cols
for_model <- meta
#select only these two cols
for_model <- meta %>% select(viral, top_topic, Party)
#one hot encodes the topic assignment
for_model$top_topic <- as.factor(for_model$top_topic)
dummy <- dummyVars(" ~ .", data=for_model)
for_model <- data.frame(predict(dummy, newdata = for_model)) %>%
filter('Party.I' == 0) %>%
select(-Party.R)
#one hot encodes the topic assignment
for_model$top_topic <- as.factor(for_model$top_topic)
dummy <- dummyVars(" ~ .", data=for_model)
for_model <- data.frame(predict(dummy, newdata = for_model)) %>%
filter('Party.I' != 0) %>%
select(-Party.R)
#select only these two cols
for_model <- meta %>% select(viral, top_topic, Party)
#one hot encodes the topic assignment
for_model$top_topic <- as.factor(for_model$top_topic)
dummy <- dummyVars(" ~ .", data=for_model)
for_model <- data.frame(predict(dummy, newdata = for_model)) %>%
filter('Party.I' != 0) %>%
select(-Party.R)
#select only these two cols
for_model <- meta %>% select(viral, top_topic, Party)
#one hot encodes the topic assignment
for_model$top_topic <- as.factor(for_model$top_topic)
dummy <- dummyVars(" ~ .", data=for_model)
for_model <- data.frame(predict(dummy, newdata = for_model)) %>%
filter('Party.I' != 0) %>%
select(-Party.R, -Party.I)
#linear model, does topic and party predict virality
model <- lm(viral~.,for_model)
#which topics are most predictive?
summary(model)
#linear model, does topic and party predict virality
model <- lm(viral~.,for_model)
#which topics are most predictive?
summary(model)
plot(stm_viral, type = "summary")
View(for_model)
View(x)
write.csv(x, '/Users/benberkman/Downloads/topics.csv')
write.csv(x, '/Users/benberkman/Downloads/topics.csv', row.names = FALSE)
View(for_model)
write.csv(for_model, '/Users/benberkman/Downloads/topics.csv', row.names = FALSE)
#Clearing the environment
rm(list = ls())
# Setting the current working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# import libraries
library(dplyr)
library(quanteda)
library(quanteda.corpora)
library(quanteda.textmodels)
library(data.table)
library(readtext)
library(caret)
library(glmnet)
# read in file
ukraine <- read.csv("../data/ukraine_tweets.csv")
ukraine$text <- gsub(pattern = "'", "", ukraine$text)  # replace apostrophes
#format as date, remove Independents
ukraine <- ukraine %>% mutate(created_at = as.Date(created_at, format = "%Y-%m-%d")) %>%
filter(grepl('D|R', Party))
### Naive Bayes
# split sample into training & test sets
set.seed(1984L)
prop_train <- 0.9 # open to change this
# Save the indexes
ids <- 1:nrow(ukraine)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
ids_test <- ids[-ids_train]
train_set <- ukraine[ids_train,]
test_set <- ukraine[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english"))
# smoothing doesn't help
nb <- textmodel_nb(train_dfm, train_set$viral, smooth = 0, prior = "uniform")
# match test set dfm to train set dfm features
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# evaluate on test set
predicted_class <- predict(nb, newdata = test_dfm, force=TRUE)
# get confusion matrix
cmat <- table(test_set$viral, predicted_class)
acc <- sum(diag(cmat))/sum(cmat) # accuracy = (TP + TN) / (TP + FP + TN + FN)
recall <- cmat[2,2]/sum(cmat[2,]) # recall = TP / (TP + FN)
precision <- cmat[2,2]/sum(cmat[,2]) # precision = TP / (TP + FP)
f1 <- 2*(recall*precision)/(recall + precision)
# print
cat(
# "Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  acc, "\n",
"Recall:",  recall, "\n",
"Precision:",  precision, "\n",
"F1-score:", f1
)
?lm
